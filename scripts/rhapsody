#!/usr/bin/env python3
import os
import time
from decimal import Decimal
from tqdm import tqdm
import pandas as pd
import numpy as np
from biom import load_table, Table
from biom.util import biom_open
from skbio.stats.composition import clr, centralize, closure
from skbio.stats.composition import clr_inv as softmax
import matplotlib.pyplot as plt
from scipy.stats import entropy, spearmanr
import click
from scipy.sparse import csr_matrix
import datetime
from rhapsody.multimodal import MMvec
from rhapsody.util import (onehot, rank_hits, random_multimodal,
                           split_tables, format_params)


@click.group()
def rhapsody():
    pass


@rhapsody.command()
@click.option('--exog-features',
              help='Input sequence counts to use for prediction.')
@click.option('--endog-features',
              help=('Output sequence abundances to predict '
                    'from input sequence counts.'))
@click.option('--metadata-file', default=None,
              help='Input sample metadata file')
@click.option('--training-column',
              help=('Column in the sample metadata specifying which '
                    'samples are for training and testing.'),
              default=None)
@click.option('--num-testing-examples',
              help=('Number of samples to randomly select for testing'),
              default=10)
@click.option('--min-feature-count',
              help=('Minimum number of samples a microbe needs to be observed '
                    'in order to not filter out'),
              default=10)
@click.option('--epochs',
              help=('Number of epochs to train, one epoch represents the '
                    'number of samples to process an entire dataset.'), default=10)
@click.option('--batch-size',
              help='Number of samples to analyze per iteration.', default=10)
@click.option('--subsample-size',
              help='Number of sequences to analyze per sample.', default=100)
@click.option('--mc-samples',
              help='Number of Monte Carlo samples to estimate gradient steps.',
              default=5)
@click.option('--latent-dim',
              help=('Dimensionality of shared latent space. '
                    'This is analogous to the number of PC axes.'),
              default=3)
@click.option('--arm-the-gpu', is_flag=True,
              help=('Enables GPU support'),
              default=False)
@click.option('--learning-rate',
              help=('Gradient descent learning rate.'),
              default=1e-1)
@click.option('--decay-rate',
              help=('Gradient descent decay rate.'),
              default=0.1)
@click.option('--step-size',
              help=('Number of epochs before learning rate is decremented.'),
              default=1)
@click.option('--beta1',
              help=('Gradient decay rate for first Adam momentum estimates'),
              default=0.9)
@click.option('--beta2',
              help=('Gradient decay rate for second Adam momentum estimates'),
              default=0.95)
@click.option('--seed',
              help=('Random seed for the sake of reproducibility (optional).'),
              default=None)
@click.option('--checkpoint-interval',
              help=('Number of seconds before a storing a checkpoint.'),
              default=1000)
@click.option('--summary-interval',
              help=('Number of seconds before a storing a summary.'),
              default=1000)
@click.option('--summary-dir', default='summarydir',
              help='Summary directory to save cross validation results.')
@click.option('--model-output', default=None,
              help=('Model parameter file containing microbe-metabolite '
                    'interaction parameters.'))
def seq2seq(exog_features, endog_features
            metadata_file, training_column,
            num_testing_examples, min_feature_count,
            epochs, batch_size, subsample_size, mc_samples,
            latent_dim, arm_the_gpu, learning_rate,
            decay_rate, step_size, beta1, beta2, seed,
            checkpoint_interval, summary_interval,
            summary_dir, model_output):
    input_seq_counts, output_seq_counts = exog_features, endog_features
    # set random seed if specified
    if seed is not None:
        np.random.seed(seed)
        torch.manual_seed(seed)

    # TODO: rename from microbes/metabolites to something more general
    microbes = load_table(input_seq_counts)
    metabolites = load_table(output_seq_counts)

    if metadata_file is not None:
        metadata = pd.read_table(metadata_file, index_col=0)
    else:
        metadata = None

    res = split_tables(
        microbes, metabolites,
        metadata=metadata, training_column=training_column,
        num_test=num_testing_examples,
        min_samples=min_feature_count)

    (train_microbes_df, test_microbes_df,
     train_metabolites_df, test_metabolites_df) = res

    # filter out low abundance microbes
    microbe_ids = microbes.ids(axis='observation')
    metabolite_ids = metabolites.ids(axis='observation')

    params = []

    sname = '_'.join(['log_PC(%d)' % latent_dim,
                      'mc(%d)' % mc_samples,
                      'samp(%d)' % batch_size,
                      'sub(%d)' % subsample_size,
                      'lr(%s)' % "{:.2E}".format(Decimal(learning_rate)),
                      'decay(%s)' % "{:.2E}".format(Decimal(decay_rate)),
                      'beta1(%.2f)' % beta1,
                      'beta2(%.2f)' % beta2])

    sname = os.path.join(summary_dir, sname)

    trainX = csr_matrix(train_microbes_df.values)
    testX = csr_matrix(test_microbes_df.values)
    trainY = train_metabolites_df.values
    testY = test_metabolites_df.values

    n, d1 = trainX.shape
    n, d2 = trainY.shape

    if arm_the_gpu:
        # pick out the first GPU
        device_name='cuda'
    else:
        device_name='cpu'

    model = MMvec(num_microbes=d1, num_metabolites=d2, latent_dim=latent_dim,
                  batch_size=batch_size, subsample_size=subsample_size,
                  device=device_name)
    fitted_model, losses = model.fit(
        trainX, trainY, testX, testY,
        epochs=epochs, gamma=decay_rate,
        learning_rate=learning_rate, mc_samples=mc_samples,
        beta1=beta1, beta2=beta2, step_size=step_size,
        summary_interval=summary_interval,
        checkpoint_interval=checkpoint_interval)

    # now extract the model weights and save them to disk
    mu_u = fitted_model.embeddings.weight.detach().numpy()
    mu_ub = fitted_model.bias.weight.detach().numpy().reshape(-1, 1)
    mu_v = fitted_model.muV.detach().numpy()
    mu_vb = fitted_model.muVb.detach().numpy().reshape(-1, 1)

    std_u = np.exp(fitted_model.logstdU.weight.detach().numpy())
    std_ub = np.exp(
        fitted_model.logstdUb.weight.detach().numpy()
    ).reshape(-1, 1)
    std_v = np.exp(fitted_model.logstdV.detach().numpy())
    std_vb = np.exp(
        fitted_model.logstdVb.detach().numpy()
    ).reshape(-1, 1)

    otu_ids = list(map(lambda x: 'clr(%s)' % x,
                       list(train_microbes_df.columns)))
    ms_ids = list(train_metabolite_df.columns)
    pc_ids = ['PC%d' % i for i in range(len(u.shape[1]))]
    alr_ms_ids = ['log(%s:%s)' % (ms_ids[i], ms_ids[0])
                  for i in range(1, len(ms_ids))]

    df = pd.concat(
        (
            format_params(mu_u, std_u, pc_ids, otu_ids, 'microbe'),
            format_params(mu_v, std_v, pc_ids, alr_ms_ids, 'metabolite'),
            format_params(mu_ub, std_ub, ['bias'], otu_ids, 'microbe'),
            format_params(mu_vb, std_vb, ['bias'], alr_ms_ids, 'metabolite')
        ), axis=0)

    df.to_csv(model_output)


if __name__ == '__main__':
    rhapsody()
